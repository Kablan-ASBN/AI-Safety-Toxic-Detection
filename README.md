# AI-Safety System: Toxic Content Detection

## Overview
This project aims to detect and mitigate harmful text using **NLP & BERT**, deployed via **FastAPI**.
It is built to enhance **AI Safety**, ensuring responsible language model usage.

## Tech Stack Used:
**Python** (Numpy, pandas, scikit-learn)
**NLP** (Hugging Face Transformers, BERT)
**Machine Learning** (Pytorch)
**API Deployment** (FastAPI) "Still working on it"
**Cloud Hosting** (Render) "Still working on it"

## Dataset
I use the **Jigsaw Specialized Rater Pools Dataset** from [Kaggle](https://www.kaggle.com/datasets/google/jigsaw-specialized-rater-pools-dataset/data).  
The dataset contains **toxic and non-toxic comments**, labeled for identity attacks, insults, threats, and obscenity.

## Features
**Preprocessing Pipeline** (Text Cleaning, Tokenization)  
**BERT-based Classification Model** (Binary classification: toxic vs. non-toxic)
**API for Real-Time Classification** (Using FastAPI)  
**Evaluation Metrics** (Accuracy, Precision, Recall, F1-score)
**Deployed API Link**: _Coming Soon_ :-)